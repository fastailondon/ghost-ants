{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Techniques in NLP\n",
    "\n",
    "Data augmentation is commonly used in computer vision. In vision, you can almost certainly flip, rotate, or mirror an image without risk of changing the original label. However, in natural language processing (NLP), the story is totally different. Changing one word has the potential to change the meaning of the entire sentence. So we can’t come up with easy rules for data augmentation. Or can we?\n",
    "\n",
    "[Jason Wei and Kai Zou](https://arxiv.org/pdf/1901.11196.pdf) presented **EDA**: **E**asy **D**ata **A**ugmentation techniques for boosting performance on text classification tasks (for a quick implementation, see the [EDA Github repository](https://github.com/jasonwei20/eda_nlp)). EDA consists of four simple operations that do a surprisingly good job at preventing overfitting and helping train more robust models. Here they are:\n",
    "\n",
    "* **Synonym Replacement**: Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n",
    "* **Random Insertion**: Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n",
    "* **Random Swap**: Randomly choose two words in the sentence and swap their positions. Do this n times.\n",
    "* **Random Deletion**: Randomly remove each word in the sentence with probability p.\n",
    "\n",
    "\n",
    "<p align=\"center\"> <img src=\"https://d3i71xaburhd42.cloudfront.net/28e30b4b5cd511f64b3bb3d7d0f57e067b3977be/4-Table3-1.png\" alt=\"drawing\" width=\"400\" class=\"center\"> </p>\n",
    "\n",
    "Example:\n",
    "\n",
    "<p align=\"center\"> <img src=\"https://miro.medium.com/max/778/1*y88F2-lpLQNxw_ubWoGctQ.png\" alt=\"drawing\" width=\"400\" class=\"center\"> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'eda_nlp'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 379 (delta 18), reused 1 (delta 0), pack-reused 349\u001b[K\n",
      "Receiving objects: 100% (379/379), 20.41 MiB | 21.99 MiB/s, done.\n",
      "Resolving deltas: 100% (181/181), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jasonwei20/eda_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda_nlp.code.eda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_replacement = 1\n",
    "random_insertion = 1\n",
    "random_swap = 0\n",
    "random_delection = 1\n",
    "\n",
    "def apply_text_augmentation(sentences, alpha=0.1, num_aug=9):\n",
    "    \"\"\"\n",
    "    Generate more data with standard augmentation.\n",
    "\n",
    "    arguments:\n",
    "        sentences -- List[tuple(label:int, text:str)]\n",
    "        alpha -- Change probability. How much to change each sentence (default 0.1).\n",
    "        num_aug -- number of augmented sentences to generate per original sentence (default 9).\n",
    "    \n",
    "    output:\n",
    "        augmented_sentences -- List[tuple(label:int, text:str)]    \n",
    "    \"\"\"\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    for label, text in sentences:\n",
    "        \n",
    "        aug_sentences = eda(text, \n",
    "                            alpha_sr=alpha * synonym_replacement, \n",
    "                            alpha_ri=alpha * random_insertion, \n",
    "                            alpha_rs=alpha * random_swap, \n",
    "                            p_rd=alpha * random_delection, \n",
    "                            num_aug=num_aug)\n",
    "        \n",
    "        for aug_sentence in aug_sentences:\n",
    "            augmented_sentences.append((label, aug_sentence))\n",
    "\n",
    "    return augmented_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'this an interesting of data augmentation'),\n",
       " (1, 'this is an interesting example of data point augmentation'),\n",
       " (1, 'this worry is an interesting example of data augmentation'),\n",
       " (1, 'this is interesting an example of data augmentation'),\n",
       " (1, 'this an interesting example augmentation'),\n",
       " (1, 'this is an interesting example of data augmentation'),\n",
       " (1,\n",
       "  'simple text editing techniques can make brobdingnagian operation gains for small datasets'),\n",
       " (1,\n",
       "  'dim witted text redact techniques can make huge performance gains for small datasets'),\n",
       " (1,\n",
       "  'text simple editing techniques can make huge performance gains for small datasets'),\n",
       " (1,\n",
       "  'simple text editing techniques can gains huge performance make for small datasets'),\n",
       " (1,\n",
       "  'text editing techniques can make huge performance gains for small datasets'),\n",
       " (1,\n",
       "  'simple text editing techniques can make huge performance gains for small datasets '),\n",
       " (2,\n",
       "  'superscript a sad superior human comedy played out on the back road of our drollery lifes'),\n",
       " (2,\n",
       "  'a sad superior human comedy playact played out on distressing the back road of our lifes'),\n",
       " (2, 'a superior human comedy out on the back road'),\n",
       " (2,\n",
       "  'a sad superior human comedy encounter out on the back route of our lifes'),\n",
       " (2, 'a sad superior human comedy our out on the back road of played lifes'),\n",
       " (2, 'a sad superior human comedy played out on the back road of our lifes ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_text_augmentation([\n",
    "         (1,\"This is an interesting example of data augmentation\"),\n",
    "         (1,\"Simple text editing techniques can make huge performance gains for small datasets.\"),\n",
    "         (2,\"A sad, superior human comedy played out on the back road of our lifes.\")], \n",
    "    alpha=0.2, num_aug=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it really work?\n",
    "\n",
    "Although some generated sentences may be a little nonsensical, inducing some amount of noise into the dataset can be extremely helpful for training a more robust model, especially when training on smaller datasets. \n",
    "\n",
    "As shown in [the paper], using EDA outperforms normal training at almost all dataset sizes over 5 benchmark text classification tasks, and does way better when training on small amounts of data. On average, training a recurrent neural network (RNN) **with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data**.\n",
    "\n",
    "<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/jasonwei20/eda_nlp/master/eda_figure.png\" alt=\"drawing\" width=\"400\" class=\"center\"> </p>\n",
    "\n",
    "\n",
    "\n",
    "## How much improvement can we expect?\n",
    "\n",
    "You will not se miracles in terms of performance improvement. Authors described the performance gain for small datasets being around 2–3% and modest for larger sizes (~1%). However, the best benefit here it to achieve competitive results with less real data. \n",
    "\n",
    "## How much augmentation?\n",
    "\n",
    "How many augmented sentences should we generate for the real sentence? \n",
    "\n",
    "The answer for this depends on the size of your dataset. If you only have a small dataset, overfitting is more likely so you can generate a larger number of augmented sentences. For larger datasets, adding too much augmented data can be unhelpful since your model may already be able to generalize when there is a large amount of real data. This figure shows performance gain with respect to the number of augmented sentences generated per original sentence:\n",
    "\n",
    "<p align=\"center\"> <img src=\"https://miro.medium.com/max/842/1*eKvsUdhBS3cCwsqdu9G0JA.png\" alt=\"drawing\" width=\"400\" class=\"center\"> </p>\n",
    "\n",
    "\n",
    "## What next?\n",
    "\n",
    "Simple text editing techniques can make huge performance gains for small datasets.\n",
    "We’ve shown that simple data augmentation operations can significantly boost performance in text classification. If you are training a text classifier on a small dataset and looking for an easy way to get better performance.\n",
    "\n",
    "## Credits\n",
    "\n",
    "By Arian Pasquali, 100% based on Jason Wei and Kai Zou work and Jason Wei blog post.\n",
    "\n",
    "* Code https://github.com/jasonwei20/eda_nlp\n",
    "* Paper https://arxiv.org/abs/1901.11196\n",
    "\n",
    "## fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
